human_code_dataset = [
    {
        "code": """
def add_numbers(a, b):
    \"\"\"Returns the sum of a and b.\"\"\"
    return a + b
"""
    },
    {
        "code": """
def factorial(n):
    \"\"\"Returns the factorial of n.\"\"\"
    if n == 0:
        return 1
    else:
        return n * factorial(n-1)
"""
    },
    {
        "code": """
def is_prime(num):
    \"\"\"Returns True if num is a prime number.\"\"\"
    if num <= 1:
        return False
    for i in range(2, int(num ** 0.5) + 1):
        if num % i == 0:
            return False
    return True
"""
    }
]
machine_code_dataset = [
    {
        "code": """
def add_numbers(a, b):
    \"\"\"This function adds two numbers.\"\"\"
    sum = a + b
    return sum
"""
    },
    {
        "code": """
def factorial(n):
    \"\"\"Calculates the factorial of a number.\"\"\"
    result = 1
    for i in range(1, n + 1):
        result *= i
    return result
"""
    },
    {
        "code": """
def is_prime(num):
    \"\"\"Check if a number is prime.\"\"\"
    if num < 2:
        return False
    for i in range(2, num):
        if num % i == 0:
            return False
    return True
"""
    }
]
from datasets import Dataset
from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments

# Convert lists to Dataset objects
human_code_dataset = Dataset.from_dict({"code": [item["code"] for item in human_code_dataset]})
machine_code_dataset = Dataset.from_dict({"code": [item["code"] for item in machine_code_dataset]})

# Initialize the tokenizer and model
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token
model = GPT2LMHeadModel.from_pretrained('gpt2')

# Tokenize the datasets with labels
def tokenize_function(examples):
    tokenized_inputs = tokenizer(examples['code'], padding='max_length', truncation=True)
    tokenized_inputs["labels"] = tokenized_inputs["input_ids"].copy()
    return tokenized_inputs

tokenized_human_code_dataset = human_code_dataset.map(tokenize_function, batched=True)
tokenized_machine_code_dataset = machine_code_dataset.map(tokenize_function, batched=True)

# Define training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=2,  # Small batch size for small dataset
    per_device_eval_batch_size=2,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

# Create Trainer instances
trainer_human_code = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_human_code_dataset,
)

trainer_machine_code = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_machine_code_dataset,
)

# Train the models
trainer_human_code.train()
trainer_human_code.save_model('./human_code_model')

# Load a fresh model for machine code fine-tuning
model_machine_code = GPT2LMHeadModel.from_pretrained('gpt2')
trainer_machine_code = Trainer(
    model=model_machine_code,
    args=training_args,
    train_dataset=tokenized_machine_code_dataset,
)

# Train the machine code model
trainer_machine_code.train()
trainer_machine_code.save_model('./machine_code_model')

# Generate code with the separately trained models
input_prompt = "def add_numbers(a, b):"
inputs = tokenizer(input_prompt, return_tensors="pt")

# Load the human code model for generation
model_human_code = GPT2LMHeadModel.from_pretrained('./human_code_model')
generated_human_code = model_human_code.generate(**inputs)
print("Human Code Model Output:")
print(tokenizer.decode(generated_human_code[0], skip_special_tokens=True))

# Load the machine code model for generation
model_machine_code = GPT2LMHeadModel.from_pretrained('./machine_code_model')
generated_machine_code = model_machine_code.generate(**inputs)
print("Machine Code Model Output:")
print(tokenizer.decode(generated_machine_code[0], skip_special_tokens=True))

