import json
import requests
import re
from bs4 import BeautifulSoup

# for parsing csv files with large field sizes
import sys
import csv
maxInt = sys.maxsize
while True:
    # decrease the maxInt value by factor 10 
    # as long as the OverflowError occurs.
    try:
        csv.field_size_limit(maxInt)
        break
    except OverflowError:
        maxInt = int(maxInt/10)

class _Scraper:
    # given a github url in the form https://github.com/<user>/<project>/commit/<commit_id> returns the raw url of the file after the commit
    def scrapeRawURL(self, url: str):
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')

        for a in soup.find_all('a', href=True):
            if "data-hotkey" in a.attrs and a['data-hotkey'] == "p":
                commit_hash = a['href'].split("/")[-1]

            if "data-ga-click" in a.attrs and "View file" in a['data-ga-click']:
                url = "https://github.com" + a['href']

        parts = url.split("/")
        parts[6] = commit_hash
        parts[5] = "raw"
        raw_url = "/".join(parts)
        return raw_url
    # returns the data in a generic file
    def processFile(self, filename):
        data = []
        match filename.split(".")[-1]:
            case "csv":
                with open(filename, encoding="utf-8") as f:
                    c = csv.DictReader(f)
                    for line in c:
                        data.append(line)
            case "json":
                with open(filename, "r") as f:
                    for line in f:
                        data.append(json.loads(line))
        return data

class BigVul(_Scraper):
    def __init__(self, filename):
        self.data = self.processFile(filename)

    # returns the url of the raw file where the vulnerability is present
    def getRawURL(self, index: int):
        # make sure the index is valid based on the supplied data
        if index < 0 or index >= len(self.data):
            raise Exception("error30")
        
        url = self.data[index]["ref_link"]
        return self.scrapeRawURL(url)

class DiverseVul(_Scraper):
    def __init__(self, filename):
        self.data = self.processFile(filename)

    # returns the url of the raw file where the vulnerability is present
    def getRawURL(self, index: int):
        # make sure the index is valid based on the supplied data
        if index < 0 or index >= len(self.data):
            raise Exception("error73")
        row = self.data[index]
        commit_hash = row["commit_id"]
        url = f"https://github.com/{row['project']}/{row['project']}/commit/{row['commit_id']}"
        return self.scrapeRawURL(url)

c = DiverseVul("../input/diversevul.json")
print(c.getRawURL(5))