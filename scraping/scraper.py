import json
import requests
import re
from bs4 import BeautifulSoup

# for parsing csv files with large field sizes
import sys
import csv
maxInt = sys.maxsize
while True:
    # decrease the maxInt value by factor 10 
    # as long as the OverflowError occurs.
    try:
        csv.field_size_limit(maxInt)
        break
    except OverflowError:
        maxInt = int(maxInt/10)

class _Scraper:
    # given a github url in the form https://github.com/<user>/<project>/commit/<commit_id> returns the raw url of the file after the commit
    def scrapeRawURL(self, url: str):
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')

        # check for "1 changed file"
        if "1 changed file" not in r.text:
            print(f"error: {url} doesn't have 1 changed file")
            return None

        for a in soup.find_all('a', href=True):
            if "data-hotkey" in a.attrs and a['data-hotkey'] == "p":
                commit_hash = a['href'].split("/")[-1]

            if "data-ga-click" in a.attrs and "View file" in a['data-ga-click']:
                url = "https://github.com" + a['href']

        parts = url.split("/")
        parts[6] = commit_hash
        parts[5] = "raw"
        raw_url = "/".join(parts)
        return raw_url

    # returns the data in a generic file
    def processFile(self, filename):
        data = []
        match filename.split(".")[-1]:
            case "csv":
                with open(filename, encoding="utf-8") as f:
                    c = csv.DictReader(f)
                    for line in c:
                        data.append(line)
            case "json":
                with open(filename, "r") as f:
                    for line in f:
                        data.append(json.loads(line))
        return data

class BigVul(_Scraper):
    def __init__(self, filename):
        self.data = self.processFile(filename)

    # returns the url of the raw file where the vulnerability is present
    def getRawURL(self, index: int):
        # make sure the index is valid based on the supplied data
        if index < 0 or index >= len(self.data):
            raise Exception("error30")
        
        url = self.data[index]["ref_link"]
        return self.scrapeRawURL(url)

    # adds the full file context to the data
    def augmentData(self, index: int):
        raw_url = self.getRawURL(index)
        r = requests.get(raw_url)
        self.data[index]["full_content"] = r.text

    # returns augmented info for a given index
    def getItem(self, index: int):
        if "full_content" not in self.data[index]:
            self.augmentData(index)
        return self.data[index]



# DivesrseVul might be unreliable because we don't have the user, only project name
class DiverseVul(_Scraper):
    def __init__(self, filename):
        self.data = self.processFile(filename)

    # returns the url of the raw file where the vulnerability is present
    def getRawURL(self, index: int):
        # make sure the index is valid based on the supplied data
        if index < 0 or index >= len(self.data):
            raise Exception("error73")
        row = self.data[index]
        
        url = f"https://github.com/search?q={row['commit_id']}&type=commits"
        r = requests.get(url)
        soup = BeautifulSoup(r.text, 'html.parser')
        # find first link
        suffix = ""
        for a in soup.find_all('a', href=True):
            # if it starts with \"/
            if a['href'].startswith("\\\"/"):
                suffix = a['href']
                break
        print(suffix)

        url = f"https://github.com{suffix[2:-2]}"
        return self.scrapeRawURL(url)