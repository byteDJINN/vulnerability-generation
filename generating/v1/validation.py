import os
import random
import re
import json
from dotenv import load_dotenv

import DS
from utils import chat4o, chat

from openai import OpenAI

load_dotenv()
apiKey = os.getenv("OPENAI_API_KEY")
client = OpenAI(api_key=apiKey)

cost = 0

# Function to output whether the LLM thinks the code is vulnerable or not
def isVulnerable(cwe, desc, code):
    global cost
    user = f"""
                    Does the following function contain an obvious {cwe} vulnerability: {desc}?
                    Respond with a description of the code and analysis of any checks in the code that would mitigate the vulnerability. 
                    Your last line should be a single word either "VULN" or "SAFE" to indicate your evaluation.

                    {code}
                """ 
    user = f"""
                    You are a CWE analysis tool that determines if code is vulnerable to a specific CWE.
                    The current CWE is {cwe} and the description is {desc}.
                    First, provide a technical analysis of the code snippet below. 
                    Second, provide a description of any checks in the code that would mitigate the vulnerability.
                    Finally, provide a single word indicating if the code is vulnerable or safe. The word should be either "VULN" or "SAFE".

                    Code:
                    {code}
                """ 
    response, _cost = chat(client, user)
    cost += _cost

    fcost = 0
    with open(f"cost.txt","r") as file:
        fcost = float(file.read())
    with open(f"cost.txt", "w") as file:
        file.write(str(fcost + _cost))

    
    # if unanimous say vulnerable otherwise say safe x3

    # search from the end for the word safe or vuln
    response = response.split("\n")
    response = response[::-1]
    safematch = False
    vulnmatch = False
    for line in response:
        if "SAFE" in line:
            safematch = True
            break
        if "VULN" in line:
            vulnmatch = True
            break

    if vulnmatch and not safematch:
        return 1
    if safematch and not vulnmatch:
        return 0
    print("FAILED")
    print(response)
    return -1

def _removeComments(code):
    code = re.sub(r"/\*.*?\*/", "", code, flags=re.DOTALL)
    code = re.sub(r"//.*?$", "", code, flags=re.MULTILINE)
    code = re.sub(r'\s+$', '', code, flags=re.MULTILINE)
    return code


data = DS.Handler("../../input/diversevul_3.json")
print(data.length(DS.SINGLE))

cweDB = {}
with open("../../input/2000.csv", "r", encoding="utf-8") as file:
    cweDB = {f"CWE-{line.split(',')[0]}": line.split(",")[1] for line in file.readlines()}

tp = 0
tn = 0
fp = 0
fn = 0
sv = 0
ss = 0

for i in range(200):
    cwe, vuln, safe = data.get(i, DS.SINGLE, DS.SEMANTIC)

    if cwe not in cweDB or vuln[0] is None or safe[0] is None:
        continue

    if len(vuln[0]) < 20 or len(safe[0]) < 20 or len(vuln[0]) > 4000 or len(safe[0]) > 4000:
        continue

    vuln = _removeComments(vuln[0])
    safe = _removeComments(safe[0])
    
    ret = []
    for j in range(3):
        ret.append(isVulnerable(cwe, cweDB[cwe], vuln))
    # sums and then rounds to either 0 or 1
    if sum(ret) == 3:
        tp += 1
    elif sum(ret) == 0:
        fn += 1
    else:
        sv += 1

    ret = []
    for j in range(3):
        ret.append(isVulnerable(cwe, cweDB[cwe], safe))
    # sums and then rounds to either 0 or 1
    if sum(ret) == 0:
        tn += 1
    elif sum(ret) == 3:
        fp += 1
    else:
        ss += 1

    print(f"TP: {tp}, TN: {tn}, FP: {fp}, FN: {fn}, SV: {sv}, SS: {ss}")
    print(f"TP: {tp+sv}, TN: {tn}, FP: {fp+ss}, FN: {fn}")
    print(f"Cost: {cost}")

# TP: 94, TN: 45, FP: 78, FN: 29